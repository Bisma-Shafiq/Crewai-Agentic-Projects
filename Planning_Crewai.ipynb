{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo3f+BNDRUHJjme+59NZjX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bisma-Shafiq/Crewai-Agentic-Projects/blob/main/Planning_Crewai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NHoE-_gRNgT2",
        "outputId": "95a3e7f8-3c97-4c72-ba75-91dbd9e94f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.2/240.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.9/545.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U crewai crewai-tools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"MODEL\"] = \"gemini/gemini-1.5-flash\""
      ],
      "metadata": {
        "id": "Ep5kNNSENsOL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio   # to run crews\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "LeX9IBelOIXS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai create crew project_04"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mhc5FGEuON6_",
        "outputId": "052d430e-afb6-4611-bd5b-4f674be61ecc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\u001b[1mCreating folder project_04...\u001b[0m\n",
            "\u001b[36mCache expired or not found. Fetching provider data from the web...\u001b[0m\n",
            "\r\u001b[?25lDownloading  [------------------------------------]  0/16798\r\u001b[?25lDownloading  [#################-------------------]  8192/16798\r\u001b[?25lDownloading  [###################################-]  16384/16798\r\u001b[?25lDownloading  [####################################]  24576/16798\r\u001b[?25lDownloading  [####################################]  32768/16798\r\u001b[?25lDownloading  [####################################]  40960/16798\r\u001b[?25lDownloading  [####################################]  49152/16798\r\u001b[?25lDownloading  [####################################]  57344/16798\r\u001b[?25lDownloading  [####################################]  65536/16798\r\u001b[?25lDownloading  [####################################]  73728/16798\r\u001b[?25lDownloading  [####################################]  81920/16798\r\u001b[?25lDownloading  [####################################]  90112/16798\r\u001b[?25lDownloading  [####################################]  98304/16798\r\u001b[?25lDownloading  [####################################]  106496/16798\r\u001b[?25lDownloading  [####################################]  114688/16798\r\u001b[?25lDownloading  [####################################]  122880/16798\r\u001b[?25lDownloading  [####################################]  131072/16798\r\u001b[?25lDownloading  [####################################]  139264/16798\r\u001b[?25lDownloading  [####################################]  147456/16798\r\u001b[?25lDownloading  [####################################]  155648/16798\r\u001b[?25lDownloading  [####################################]  163840/16798\r\u001b[?25lDownloading  [####################################]  172032/16798\r\u001b[?25lDownloading  [####################################]  180224/16798\r\u001b[?25lDownloading  [####################################]  188416/16798\r\u001b[?25lDownloading  [####################################]  196608/16798\r\u001b[?25lDownloading  [####################################]  204800/16798\r\u001b[?25lDownloading  [####################################]  212992/16798\r\u001b[?25lDownloading  [####################################]  221184/16798\r\u001b[?25lDownloading  [####################################]  229376/16798\r\u001b[?25lDownloading  [####################################]  237568/16798\r\u001b[?25lDownloading  [####################################]  245760/16798\r\u001b[?25lDownloading  [####################################]  253952/16798\r\u001b[?25lDownloading  [####################################]  262144/16798\r\u001b[?25lDownloading  [####################################]  270336/16798\r\u001b[?25lDownloading  [####################################]  278528/16798\r\u001b[?25lDownloading  [####################################]  286720/16798\r\u001b[?25lDownloading  [####################################]  294912/16798\r\u001b[?25lDownloading  [####################################]  303104/16798\r\u001b[?25lDownloading  [####################################]  311296/16798\r\u001b[?25lDownloading  [####################################]  319488/16798\r\u001b[?25lDownloading  [####################################]  327680/16798\r\u001b[?25lDownloading  [####################################]  335872/16798\r\u001b[?25lDownloading  [####################################]  344064/16798\r\u001b[?25lDownloading  [####################################]  349185/16798\u001b[?25h\n",
            "\u001b[36mSelect a provider to set up:\u001b[0m\n",
            "\u001b[36m1. openai\u001b[0m\n",
            "\u001b[36m2. anthropic\u001b[0m\n",
            "\u001b[36m3. gemini\u001b[0m\n",
            "\u001b[36m4. nvidia_nim\u001b[0m\n",
            "\u001b[36m5. groq\u001b[0m\n",
            "\u001b[36m6. ollama\u001b[0m\n",
            "\u001b[36m7. watson\u001b[0m\n",
            "\u001b[36m8. bedrock\u001b[0m\n",
            "\u001b[36m9. azure\u001b[0m\n",
            "\u001b[36m10. cerebras\u001b[0m\n",
            "\u001b[36m11. sambanova\u001b[0m\n",
            "\u001b[36m12. other\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 3\n",
            "\u001b[36mSelect a model to use for Gemini:\u001b[0m\n",
            "\u001b[36m1. gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[36m2. gemini/gemini-1.5-pro\u001b[0m\n",
            "\u001b[36m3. gemini/gemini-gemma-2-9b-it\u001b[0m\n",
            "\u001b[36m4. gemini/gemini-gemma-2-27b-it\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 1\n",
            "Enter your GEMINI API key (press Enter to skip): AIzaSyB6uIZyYinHhGxBgbu8SsTMMy45dtTLp2E\n",
            "\u001b[32mAPI keys and model saved to .env file\u001b[0m\n",
            "\u001b[32mSelected model: gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[32m  - Created project_04/.gitignore\u001b[0m\n",
            "\u001b[32m  - Created project_04/pyproject.toml\u001b[0m\n",
            "\u001b[32m  - Created project_04/README.md\u001b[0m\n",
            "\u001b[32m  - Created project_04/knowledge/user_preference.txt\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/main.py\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/crew.py\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/tools/custom_tool.py\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/tools/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/config/agents.yaml\u001b[0m\n",
            "\u001b[32m  - Created project_04/src/project_04/config/tasks.yaml\u001b[0m\n",
            "\u001b[32m\u001b[1mCrew project_04 created successfully!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hN9MD8U2OasC",
        "outputId": "91e87692-5b3e-4037-b787-417915c05a07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_04"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05l0K_KlO0sg",
        "outputId": "9d1c7f58-8ce2-4f6f-ef50-eec5fb68fba5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project_04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl2bA0CuO6HY",
        "outputId": "4de2ae99-8054-4a44-dcae-f6f31175c50f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowledge  pyproject.toml  README.md  src  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmHye6UTO9nw",
        "outputId": "e30e5331-7fb8-4cbc-ed2f-feb2a091d346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Crew\n",
            "\u001b[1m\u001b[93m \n",
            "[2025-03-09 04:56:44][INFO]: Planning the crew execution\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "Step 1: Define Research Scope\n",
            "The AI LLMs Senior Data Researcher will begin by clearly defining the scope of their research. This includes specifying the types of LLMs to focus on (e.g., transformer-based models, specific model architectures, etc.), the key aspects to investigate (e.g., performance metrics, ethical considerations, societal impact, emerging applications), and the relevant timeframe (2025 and recent advancements). \n",
            "\n",
            "Step 2: Identify Reliable Information Sources\n",
            "The researcher will identify and curate a list of reliable information sources. This includes academic publications (journals, conference proceedings), reputable industry reports, leading researchers' websites and publications, and pre-print servers like arXiv.  They should prioritize peer-reviewed sources and information from established organizations in the field of AI.\n",
            "\n",
            "Step 3: Comprehensive Data Gathering\n",
            "The researcher will systematically gather relevant information from the identified sources.  This will involve detailed reading and critical evaluation of the content. The researcher should take thorough notes, highlighting key findings and noting any contradictions or inconsistencies across different sources. They might utilize tools like citation management software (although none are explicitly provided, this would significantly help) for organization and reference tracking.\n",
            "\n",
            "Step 4: Analyze and Synthesize Information\n",
            "After gathering data, the researcher will analyze and synthesize the findings to identify trends, patterns, and significant advancements in the field of AI LLMs in 2025. They should critically assess the validity and reliability of the information, identifying any potential biases or limitations in the sources.\n",
            "\n",
            "Step 5: Compile the Top 10 Key Findings\n",
            "Based on their analysis, the researcher will compile a list of 10 bullet points representing the most relevant and interesting information on AI LLMs in 2025.  These bullet points should concisely summarize major advancements, important challenges, or noteworthy trends.  The focus should be on novelty and significant contributions to the field.\n",
            "\n",
            "Step 6: Review and Refine\n",
            "The researcher will thoroughly review the 10 bullet points for clarity, accuracy, and relevance.  They should ensure each point is well-supported by the evidence gathered and accurately reflects the current state of the field.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Increased focus on multimodal LLMs:**  2025 has seen a significant surge in research and development of multimodal LLMs, capable of processing and generating various data types, including text, images, audio, and video.  This trend reflects a move towards more comprehensive and human-like AI systems.  Several prominent models have emerged showcasing impressive capabilities in tasks requiring understanding and generating diverse data modalities.\n",
            "\n",
            "* **Advancements in reasoning and problem-solving:**  Significant progress has been made in enhancing the reasoning and problem-solving capabilities of LLMs.  Techniques like chain-of-thought prompting and the integration of external knowledge bases have demonstrably improved performance on complex tasks requiring logical inference and multi-step reasoning.\n",
            "\n",
            "* **Improved efficiency and scalability:**  Research efforts are increasingly focused on developing more efficient and scalable LLM architectures.  This includes exploring techniques like model quantization, pruning, and knowledge distillation to reduce computational costs and memory requirements, enabling deployment on resource-constrained devices.\n",
            "\n",
            "* **Growing emphasis on explainability and interpretability:**  The need for greater transparency and understanding of LLM decision-making processes has fueled research into explainable AI (XAI) techniques.  New methods are emerging to provide insights into the internal workings of LLMs, facilitating better debugging, trust building, and addressing potential biases.\n",
            "\n",
            "* **Enhanced control and safety mechanisms:**  Addressing safety and ethical concerns associated with LLMs remains a high priority.  Researchers are actively developing techniques to improve the controllability of LLMs, mitigating risks such as generating harmful or biased content, and promoting responsible AI development practices.\n",
            "\n",
            "* **Emergence of specialized LLMs for niche applications:**  The versatility of LLMs has spurred the development of specialized models tailored to specific domains, such as healthcare, finance, and scientific research. These domain-specific LLMs leverage specialized datasets and training techniques to achieve higher accuracy and efficiency in their respective fields.\n",
            "\n",
            "* **Integration of LLMs with other AI technologies:**  LLMs are increasingly being integrated with other AI technologies, such as computer vision, robotics, and reinforcement learning. This synergistic approach has enabled the creation of more sophisticated and powerful AI systems capable of tackling complex real-world problems.\n",
            "\n",
            "* **Addressing biases and fairness in LLMs:**  The issue of bias in LLMs is being actively addressed through techniques such as data augmentation, adversarial training, and bias mitigation algorithms.  Research is focused on developing more equitable and fair LLMs that avoid perpetuating societal biases.\n",
            "\n",
            "* **Development of more robust and resilient LLMs:**  Researchers are exploring methods to create LLMs that are more robust and resilient to adversarial attacks and noisy data.  These advancements aim to improve the reliability and trustworthiness of LLMs in real-world applications.\n",
            "\n",
            "* **Increased focus on the societal impact of LLMs:**  The increasing impact of LLMs on various aspects of society has led to a growing focus on their ethical, social, and economic implications.  Researchers and policymakers are working collaboratively to address the potential risks and opportunities presented by this transformative technology.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
            "Step 1: Receive Data from Senior Researcher\n",
            "The AI LLMs Reporting Analyst will receive the 10 bullet points summarizing key findings from the Senior Data Researcher.\n",
            "\n",
            "Step 2: Expand Each Bullet Point into a Section\n",
            "For each of the 10 bullet points, the analyst will conduct further research to expand it into a detailed section for the report.  This will involve revisiting original sources cited by the researcher, possibly exploring additional relevant information, and ensuring a comprehensive and accurate representation of each topic.  This step will require a deep understanding of the subject matter to appropriately expand on the key findings.\n",
            "\n",
            "Step 3: Structure and Organize the Report\n",
            "The analyst will structure the report logically, including an introduction, conclusion, and clear section headings for each of the expanded bullet points. A suggested structure could be: Introduction (context and overview), Section 1 (Expanded Bullet Point 1), Section 2 (Expanded Bullet Point 2), ..., Section 10 (Expanded Bullet Point 10), Conclusion (Summary and future directions).\n",
            "\n",
            "Step 4: Write Detailed Report Sections\n",
            "The analyst will write detailed and comprehensive sections for each topic, integrating relevant evidence and ensuring the information is accurate, clear, and easy to understand. They will maintain a consistent tone and style throughout the report.\n",
            "\n",
            "Step 5: Fact-Checking and Verification\n",
            "Before finalization, the analyst will rigorously fact-check all information presented in the report, ensuring the accuracy and reliability of all data and claims. This might involve cross-referencing information from multiple sources.\n",
            "\n",
            "Step 6: Review and Edit\n",
            "The analyst will review and edit the entire report to ensure clarity, coherence, and consistency. This will include checking for grammatical errors, typos, and inconsistencies in style and formatting.  They should also ensure that the report is well-structured and flows logically.\n",
            "\n",
            "Step 7: Final Formatting and Submission\n",
            "Finally, the analyst will format the report in markdown as requested, ensuring proper headings, subheadings, and any other necessary formatting elements.  The report will be reviewed one final time before submission.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# The Evolution of Large Language Models: A 2025 Progress Report\n",
            "\n",
            "**Introduction:**\n",
            "\n",
            "Large Language Models (LLMs) have undergone a period of rapid advancement in 2025, marked by significant breakthroughs in various areas. This report details key advancements across multiple facets of LLM development and deployment, examining progress in model capabilities, efficiency, safety, and societal impact.  The findings presented are based on a review of current research and publicly available information.\n",
            "\n",
            "**Section 1: The Rise of Multimodal LLMs**\n",
            "\n",
            "2025 has witnessed a substantial shift towards multimodal LLMs.  These models represent a significant leap beyond unimodal text-based systems, exhibiting the capacity to process and generate diverse data types, including text, images, audio, and video. This capability allows for a more comprehensive understanding of context and facilitates the creation of richer, more nuanced outputs.  Prominent examples include models capable of generating image captions from audio descriptions, answering questions based on visual input, and even creating synchronized audio-visual content from textual prompts. The development of efficient methods for handling multimodal data, such as cross-modal attention mechanisms and joint embedding spaces, has been crucial to this progress. This trend signals a move towards AI systems that are more closely aligned with human cognitive abilities, capable of integrating information from multiple sensory modalities.\n",
            "\n",
            "**Section 2:  Advancements in Reasoning and Problem-Solving**\n",
            "\n",
            "Significant strides have been made in enhancing the reasoning and problem-solving capabilities of LLMs.  Methods such as chain-of-thought prompting have proven remarkably effective. This technique involves prompting the model to articulate its reasoning process step-by-step, leading to improved performance on tasks requiring logical inference and multi-step reasoning.  The integration of external knowledge bases, providing LLMs access to a vast repository of structured information, has further enhanced their problem-solving capabilities, enabling them to access and utilize relevant factual information to inform their responses.  This combination of improved prompting strategies and external knowledge access represents a crucial step towards creating LLMs capable of tackling complex, real-world challenges that demand sophisticated cognitive abilities.\n",
            "\n",
            "**Section 3:  Improving Efficiency and Scalability**\n",
            "\n",
            "The development of more efficient and scalable LLM architectures remains a critical area of focus.  Techniques like model quantization, which reduces the precision of model weights, and pruning, which removes less important connections, have demonstrably reduced computational costs and memory requirements. Knowledge distillation, a method of transferring knowledge from a large, complex model to a smaller, more efficient one, further contributes to this effort. These advancements are crucial for deploying LLMs on resource-constrained devices, making them accessible to a wider range of applications and users.  The ongoing research in efficient architectures and training methods will continue to drive down the cost and energy consumption associated with running LLMs.\n",
            "\n",
            "**Section 4:  Explainability and Interpretability**\n",
            "\n",
            "The need for greater transparency and understanding of LLM decision-making processes has fueled substantial research into explainable AI (XAI) techniques.  Methods for visualizing the attention mechanisms of LLMs provide insights into which parts of the input data the model focuses on during processing. Other techniques delve into the internal representations and activation patterns of the model to reveal its underlying reasoning process.  These advancements are crucial for building trust in LLMs, identifying and mitigating biases, and improving the overall reliability and trustworthiness of these systems.  The development of robust XAI techniques will be essential for responsible and ethical deployment of increasingly powerful LLMs.\n",
            "\n",
            "**Section 5:  Enhanced Control and Safety Mechanisms**\n",
            "\n",
            "Safety and ethical concerns associated with LLMs are being actively addressed.  Researchers are actively developing techniques to improve the controllability of LLMs, focusing on mitigating risks such as the generation of harmful or biased content.  Methods like reinforcement learning from human feedback (RLHF) are being refined to align LLM behavior with human values and preferences.  The development of robust safety protocols and mechanisms is crucial for ensuring the responsible development and deployment of these powerful technologies, minimizing potential harm and promoting the ethical use of AI.\n",
            "\n",
            "**Section 6:  Specialized LLMs for Niche Applications**\n",
            "\n",
            "The versatility of LLMs has spurred the creation of specialized models tailored to specific domains, such as healthcare, finance, and scientific research. These domain-specific LLMs leverage specialized datasets and training techniques to achieve higher accuracy and efficiency within their respective fields. For example, LLMs trained on medical literature can assist in diagnosis and treatment planning, while those trained on financial data can improve risk assessment and fraud detection.  This specialization allows for more effective and targeted applications of LLM technology, maximizing its potential benefit across various industries.\n",
            "\n",
            "**Section 7:  Integration with Other AI Technologies**\n",
            "\n",
            "LLMs are increasingly being integrated with other AI technologies, creating synergistic systems that solve complex problems.  The combination of LLMs with computer vision enables AI systems to understand and interact with the visual world, while integration with robotics allows for more sophisticated and adaptable robotic control.  The incorporation of reinforcement learning enables LLMs to learn from interactions with their environment, adapting and improving their performance over time.  This convergence of AI technologies holds tremendous potential for creating truly intelligent and versatile systems.\n",
            "\n",
            "**Section 8:  Addressing Biases and Fairness**\n",
            "\n",
            "The pervasive issue of bias in LLMs is being actively addressed through various techniques. Data augmentation strategies aim to create more balanced datasets, mitigating the impact of skewed training data. Adversarial training methods expose LLMs to examples designed to reveal and challenge existing biases. Bias mitigation algorithms are being developed to directly address and correct biased outputs.  Addressing bias is a continuous process requiring ongoing research and development of increasingly sophisticated methods, crucial for ensuring fairness and equity in AI systems.\n",
            "\n",
            "**Section 9:  Developing Robust and Resilient LLMs**\n",
            "\n",
            "Research is focused on creating LLMs that are more robust and resilient to adversarial attacks and noisy data.  Techniques like adversarial training, which exposes the model to carefully designed adversarial examples, are being used to enhance its ability to withstand attempts to manipulate its outputs.  Methods for handling noisy and incomplete data are also being developed, improving the reliability and trustworthiness of LLMs in real-world applications where data quality is not always guaranteed.\n",
            "\n",
            "**Section 10:  Societal Impact and Considerations**\n",
            "\n",
            "The growing societal impact of LLMs necessitates a focus on their ethical, social, and economic implications.  Researchers, policymakers, and industry leaders are collaboratively exploring the potential risks and opportunities presented by this transformative technology.  Issues such as job displacement, misinformation, and algorithmic bias are being examined, and strategies for mitigating potential harm are being developed.  A responsible approach to LLM development and deployment requires careful consideration of these broader societal implications.\n",
            "\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "2025 marks a year of remarkable progress in LLM technology. Advancements in multimodal capabilities, reasoning, efficiency, and safety are paving the way for more powerful and beneficial AI systems.  However, ongoing research into explainability, bias mitigation, and societal impact remains crucial for responsible development and deployment. The future of LLMs promises further innovation and widespread applications, but careful consideration of ethical and societal implications will be essential to ensure these powerful tools are used for the benefit of all.\u001b[00m\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYjCO5fBPNOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}